{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Choose the filename of the uploaded file\n",
        "filename_true = \"/content/drive/MyDrive/word_data/output.txt\"\n",
        "\n",
        "# Read the content of the file line by line and store it in a NumPy array\n",
        "lines = []\n",
        "with open(filename_true, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "# Convert the list of lines to a NumPy array\n",
        "np_true = np.array(lines)\n",
        "\n",
        "filename_true_labels= \"/content/drive/MyDrive/word_data/output_label.txt\"\n",
        "lines = []\n",
        "with open(filename_true_labels, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "np_true_label= np.array(lines)\n",
        "\n",
        "\n",
        "filename_false= \"/content/drive/MyDrive/word_data/false_data.txt\"\n",
        "lines = []\n",
        "with open(filename_false, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "np_false= np.array(lines)\n",
        "\n",
        "\n",
        "filename_false_labels= \"/content/drive/MyDrive/word_data/false_data_labels.txt\"\n",
        "lines = []\n",
        "with open(filename_false_labels, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "np_false_label= np.array(lines)\n",
        "\n"
      ],
      "metadata": {
        "id": "iVdjpjjMKt5a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename_true2 = \"/content/drive/MyDrive/word_data2/output2.txt\"\n",
        "\n",
        "# Read the content of the file line by line and store it in a NumPy array\n",
        "lines = []\n",
        "with open(filename_true2, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "# Convert the list of lines to a NumPy array\n",
        "np_true2 = np.array(lines)\n",
        "\n",
        "filename_true_labels2= \"/content/drive/MyDrive/word_data2/output_label2.txt\"\n",
        "lines = []\n",
        "with open(filename_true_labels2, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "np_true_label2= np.array(lines)\n",
        "\n",
        "\n",
        "filename_false2= \"/content/drive/MyDrive/word_data2/false_data2.txt\"\n",
        "lines = []\n",
        "with open(filename_false2, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "np_false2= np.array(lines)\n",
        "\n",
        "\n",
        "filename_false_labels2= \"/content/drive/MyDrive/word_data2/false_data_labels2.txt\"\n",
        "lines = []\n",
        "with open(filename_false_labels2, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "np_false_label2= np.array(lines)"
      ],
      "metadata": {
        "id": "V5hBY2_9g2H4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X_total is your input data and y_total is your labels\n",
        "\n",
        "# Concatenate your data\n",
        "X_total = np.concatenate((np_true, np_false), axis=0)\n",
        "y_total = np.concatenate((np_true_label, np_false_label), axis=0)\n",
        "X_total2 = np.concatenate((np_true2, np_false2), axis=0)\n",
        "y_total2 = np.concatenate((np_true_label2, np_false_label2), axis=0)\n",
        "# Calculate the size of the reduced dataset (1/10)\n",
        "fraction_to_keep = 0.9  # 10% of the data\n",
        "fraction_to_keep2 = 0.9\n",
        "# Use train_test_split to randomly select a subset\n",
        "X_reduced, _, y_reduced, _ = train_test_split(X_total, y_total, test_size=1-fraction_to_keep, random_state=42)\n",
        "X_reduced2, _, y_reduced2, _ = train_test_split(X_total2, y_total2, test_size=1-fraction_to_keep2, random_state=42)\n",
        "\n",
        "y_reduced=y_reduced.astype(int)\n",
        "y_reduced2=y_reduced2.astype(int)\n",
        "# Split the reduced data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y_reduced, test_size=0.2, random_state=42)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_reduced2, y_reduced2, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_7rBAIlKYyuV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.concatenate((X_train,X_train2), axis=0)\n",
        "y_train = np.concatenate((y_train,y_train2), axis=0)\n"
      ],
      "metadata": {
        "id": "RjzXjzHnw527"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x=X_test"
      ],
      "metadata": {
        "id": "7hxy-c-whe4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4)"
      ],
      "metadata": {
        "id": "zvO2GTjLcdlq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "X_test2 = np.array([vectorize(sentence) for sentence in X_test2])\n",
        "#max_len = max(len(sentence.split()) for sentence in X_train)\n",
        "#X_train = pad_sequences(X_train, maxlen=100, padding='post')\n",
        "#X_test = pad_sequences(X_test, maxlen=100, padding='post')\n"
      ],
      "metadata": {
        "id": "FuZ6XbRgdEbD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Assuming `X_train` is your input data and `y_train` is your labels\n",
        "#max_len = max(len(sentence.split()) for sentence in X_train)\n",
        "# Text Vectorization\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Model Architecture\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a Dense layer for classification\n",
        "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "#model.add(LSTM(units=64, activation='relu'))\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test2,y_test2))\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_test_pred_binary)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_test_pred_binary)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, y_test_pred_binary)\n",
        "recall = recall_score(y_test, y_test_pred_binary)\n",
        "f1 = f1_score(y_test, y_test_pred_binary)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "# Evaluate the model\n",
        "# Assuming `X_test` is your test data and `y_test` is your test labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajn3ONb4dNlC",
        "outputId": "0327c825-efc2-4fe8-e09f-da7cdeb38f7d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "36910/36910 [==============================] - 176s 5ms/step - loss: 0.2402 - accuracy: 0.8937 - val_loss: 0.2556 - val_accuracy: 0.8853\n",
            "Epoch 2/5\n",
            "36910/36910 [==============================] - 163s 4ms/step - loss: 0.2184 - accuracy: 0.9040 - val_loss: 0.2459 - val_accuracy: 0.8902\n",
            "Epoch 3/5\n",
            "36910/36910 [==============================] - 180s 5ms/step - loss: 0.2126 - accuracy: 0.9070 - val_loss: 0.2481 - val_accuracy: 0.8898\n",
            "Epoch 4/5\n",
            "36910/36910 [==============================] - 178s 5ms/step - loss: 0.2093 - accuracy: 0.9085 - val_loss: 0.2433 - val_accuracy: 0.8919\n",
            "Epoch 5/5\n",
            "36910/36910 [==============================] - 163s 4ms/step - loss: 0.2068 - accuracy: 0.9097 - val_loss: 0.2404 - val_accuracy: 0.8933\n",
            "3443/3443 [==============================] - 12s 3ms/step\n",
            "Test Accuracy: 0.9338870733478577\n",
            "Confusion Matrix:\n",
            "[[52155  3117]\n",
            " [ 4166 50722]]\n",
            "Precision: 0.9421051654005461\n",
            "Recall: 0.9240999854248652\n",
            "F1 Score: 0.9330157182668518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5jwMTREE-Kjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(X_test)):\n",
        "  print(f\"Test:{test_x[i]} Prediction:{y_test_pred_binary[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "wagbZmsLhCCS",
        "outputId": "b149f944-473e-4a75-ae02-41bcd5b2927f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test:Bunu yapmak için site yöneticisi, genellikle her kullanıcı için bir sertifika oluşturur ve tarayıcıya yüklenen bir sertifika oluşturur.Normalde bu, yetkili kullanıcının adını ve e-posta adresini içerir ve kullanıcının kimliğini doğrulamak için, muhtemelen bir parola bile girmeden, her yeniden bağlantıda sunucu tarafından otomatik olarak kontrol edilir. Prediction:[1]\n",
            "Test:Bu tarihi izleyen yıllarda yöredeki kentleri yerle bir eden şiddetli bir depremden sonra Iğdır şehri bugünkü yerinde 1664'te kuruldu. Prediction:[1]\n",
            "Test:Mutlaki ve meşruti bir hükumetde Prediction:[1]\n",
            "Test:Osmanlı Devleti’nin kuruluş aşamasında da Bâtınîler, öncelikle askeri kuvvetlerin içerisinde yer almayı ihmâl etmediler. Prediction:[1]\n",
            "Test:Bu miktarın büyük çoğunluğu Dublin Liman Tüneline, yedi Luas projesine, iki Dublin metro hattına, DART uzantılarına ve bütün servisleri St Stephen's Green’de bir araya getirecek bir yeraltı istasyonuna harcanacaktır. Prediction:[1]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 5 is out of bounds for axis 0 with size 5",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3668e1762f6f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test:{test_x[i]} Prediction:{y_test_pred_binary[i]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings_list = [\"Sizde mi geleceksiniz\", \"Bütün eşyalar arabada duruyor\", \"kitaplarınızı çantanızda taşıyın\",\n",
        "                \"sınava hesap makineside getirebilirsiniz\", \"Bilgisayarımda antivirüs programı yüklü\"\n",
        "                ,\"Kenan abide aşı olmuş\",\"kedilerde havada uçamazmış\",\"adamı uyurkende rahatsız etme\",\"kaleminde senin gibi güzel\",\n",
        "                \"banada şu eriklerden versene\",\"Bu işde sana göre değilmiş\",\n",
        "                \"Datasetleri güzel seçelimde yapay zeka doğru çalışsın\",\"Bilgisayarın ekranınıda kırmış\",\n",
        "                \"Yemekden sonra kahvemide içtim\",\"ellerini yıkadıktan sonra dişlerinide fırçala\",\n",
        "                \"Gelecek etkinliklerimizede sizi bekleriz\",\"benimlede dışarı çıkar mısın\"\n",
        "                ,\"Televizyon izlemektende sıkılmış\",\"Bu borsa işlerine bizide bulaştırdı\",\"arkadaşına yaptıklarına bende inanamadım\"]\n",
        "\n",
        "# Create a NumPy array from the list of strings\n",
        "test_labels=[0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "test_labels=np.array(test_labels)\n",
        "my_sentences = np.array(strings_list)"
      ],
      "metadata": {
        "id": "K8oT5e1SkNIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_my_sentences= np.array([vectorize(sentence) for sentence in my_sentences])\n",
        "y_test_pred = model.predict(test_my_sentences)\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "accuracy=accuracy_score(test_labels,y_test_pred_binary)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "# Calculate accuracy\n",
        "for i in range(len(my_sentences)):\n",
        "  print(f\"Test:{my_sentences[i]} Prediction:{y_test_pred_binary[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htZ3eKuflaxt",
        "outputId": "46b13c97-f6ac-4325-939b-4f955bd85a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "Test Accuracy: 0.85\n",
            "Test:Sizde mi geleceksiniz Prediction:[0]\n",
            "Test:Bütün eşyalar arabada duruyor Prediction:[1]\n",
            "Test:kitaplarınızı çantanızda taşıyın Prediction:[0]\n",
            "Test:sınava hesap makineside getirebilirsiniz Prediction:[0]\n",
            "Test:Bilgisayarımda antivirüs programı yüklü Prediction:[1]\n",
            "Test:Kenan abide aşı olmuş Prediction:[1]\n",
            "Test:kedilerde havada uçamazmış Prediction:[0]\n",
            "Test:adamı uyurkende rahatsız etme Prediction:[0]\n",
            "Test:kaleminde senin gibi güzel Prediction:[0]\n",
            "Test:banada şu eriklerden versene Prediction:[0]\n",
            "Test:Bu işde sana göre değilmiş Prediction:[0]\n",
            "Test:Datasetleri güzel seçelimde yapay zeka doğru çalışsın Prediction:[0]\n",
            "Test:Bilgisayarın ekranınıda kırmış Prediction:[0]\n",
            "Test:Yemekden sonra kahvemide içtim Prediction:[0]\n",
            "Test:ellerini yıkadıktan sonra dişlerinide fırçala Prediction:[0]\n",
            "Test:Gelecek etkinliklerimizede sizi bekleriz Prediction:[0]\n",
            "Test:benimlede dışarı çıkar mısın Prediction:[1]\n",
            "Test:Televizyon izlemektende sıkılmış Prediction:[0]\n",
            "Test:Bu borsa işlerine bizide bulaştırdı Prediction:[0]\n",
            "Test:arkadaşına yaptıklarına bende inanamadım Prediction:[0]\n"
          ]
        }
      ]
    }
  ]
}