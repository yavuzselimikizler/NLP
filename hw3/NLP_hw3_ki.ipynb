{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pN5Y3lo8Fea7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Choose the filename of the uploaded file\n",
        "filename_true = \"/content/drive/MyDrive/word_data6/true_input4.txt\"\n",
        "\n",
        "# Read the content of the file line by line and store it in a NumPy array\n",
        "lines = []\n",
        "with open(filename_true, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "# Convert the list of lines to a NumPy array\n",
        "np_true = np.array(lines)\n",
        "\n",
        "filename_true_labels= \"/content/drive/MyDrive/word_data6/output_label4.txt\"\n",
        "lines = []\n",
        "with open(filename_true_labels, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "np_true_label= np.array(lines)\n",
        "\n",
        "\n",
        "filename_false= \"/content/drive/MyDrive/word_data6/false_data4.txt\"\n",
        "lines = []\n",
        "with open(filename_false, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "np_false= np.array(lines)\n",
        "\n",
        "\n",
        "filename_false_labels= \"/content/drive/MyDrive/word_data6/false_data_labels4.txt\"\n",
        "lines = []\n",
        "with open(filename_false_labels, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "np_false_label= np.array(lines)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "nkeE1EEMA-G-",
        "outputId": "c6c558c6-718c-4b0b-e17d-906a74f19d67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename_true2 = \"/content/drive/MyDrive/word_data5/true_input3.txt\"\n",
        "\n",
        "# Read the content of the file line by line and store it in a NumPy array\n",
        "lines = []\n",
        "with open(filename_true2, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "# Convert the list of lines to a NumPy array\n",
        "np_true2 = np.array(lines)\n",
        "\n",
        "filename_true_labels2= \"/content/drive/MyDrive/word_data5/output_label3.txt\"\n",
        "lines = []\n",
        "with open(filename_true_labels2, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "np_true_label2= np.array(lines)\n",
        "\n",
        "\n",
        "filename_false2= \"/content/drive/MyDrive/word_data5/false_data3.txt\"\n",
        "lines = []\n",
        "with open(filename_false2, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "np_false2= np.array(lines)\n",
        "\n",
        "\n",
        "filename_false_labels2= \"/content/drive/MyDrive/word_data5/false_data_labels3.txt\"\n",
        "lines = []\n",
        "with open(filename_false_labels2, 'r') as file:\n",
        "    for line in file:\n",
        "        lines.append(line.strip())\n",
        "\n",
        "np_false_label2= np.array(lines)"
      ],
      "metadata": {
        "id": "mlD_ij8dFngo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X_total is your input data and y_total is your labels\n",
        "\n",
        "subset_size = len(np_false) // 10\n",
        "random_indices = np.random.choice(len(np_false), size=subset_size, replace=False)\n",
        "subset_false = np_false[random_indices]\n",
        "\n",
        "# Concatenate np_true with the randomly chosen subset of np_false\n",
        "X_total = np.concatenate((np_true, subset_false), axis=0)\n",
        "y_total = np.concatenate((np_true_label, np_false_label[random_indices]), axis=0)\n",
        "\n",
        "# Randomly choose a subset of np_false2\n",
        "subset_size2 = len(np_false2) // 10\n",
        "random_indices2 = np.random.choice(len(np_false2), size=subset_size2, replace=False)\n",
        "subset_false2 = np_false2[random_indices2]\n",
        "\n",
        "# Concatenate np_true2 with the randomly chosen subset of np_false2\n",
        "X_total2 = np.concatenate((np_true2, subset_false2), axis=0)\n",
        "y_total2 = np.concatenate((np_true_label2, np_false_label2[random_indices2]), axis=0)\n",
        "# Calculate the size of the reduced dataset (1/10)\n",
        "fraction_to_keep = 0.95  # 10% of the data\n",
        "fraction_to_keep2 = 0.95\n",
        "# Use train_test_split to randomly select a subset\n",
        "X_reduced, _, y_reduced, _ = train_test_split(X_total, y_total, test_size=1-fraction_to_keep, random_state=42)\n",
        "X_reduced2, _, y_reduced2, _ = train_test_split(X_total2, y_total2, test_size=1-fraction_to_keep2, random_state=42)\n",
        "\n",
        "y_reduced=y_reduced.astype(int)\n",
        "y_reduced2=y_reduced2.astype(int)\n",
        "# Split the reduced data into training and testing sets\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_reduced, y_reduced, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced2, y_reduced2, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "wrMkgOCyFqO9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train= X_reduced2\n",
        "y_train = y_reduced2"
      ],
      "metadata": {
        "id": "kSsp__wZNin6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.concatenate((X_train,X_train2), axis=0)\n",
        "y_train = np.concatenate((y_train,y_train2), axis=0)"
      ],
      "metadata": {
        "id": "1efEHOLRF1V3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "w2v_model = Word2Vec(sentences, window=5, min_count=5, workers=4)"
      ],
      "metadata": {
        "id": "cG7lC80XF2RS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(sentence):\n",
        "    words = sentence.split()\n",
        "    words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "    if len(words_vecs) == 0:\n",
        "        return np.zeros(100)\n",
        "    words_vecs = np.array(words_vecs)\n",
        "    return words_vecs.mean(axis=0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])\n",
        "X_test2 = np.array([vectorize(sentence) for sentence in X_test2])\n",
        "#max_len = max(len(sentence.split()) for sentence in X_train)\n",
        "#X_train = pad_sequences(X_train, maxlen=100, padding='post')\n",
        "#X_test = pad_sequences(X_test, maxlen=100, padding='post')"
      ],
      "metadata": {
        "id": "1fHw0mEGF6Rh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "# Assuming `X_train` is your input data and `y_train` is your labels\n",
        "\n",
        "# Define early stopping criteria\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Model Architecture\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding layer if your input is text data\n",
        "# model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_len))\n",
        "\n",
        "# Add a Bidirectional LSTM layer\n",
        "model.add(Bidirectional(LSTM(units=64, activation='relu'), input_shape=(X_train.shape[1], 1)))  # Assuming X_train is a 2D array\n",
        "\n",
        "# Add a Dense layer for classification\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test2, y_test2), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_test_pred = model.predict(X_test)\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_test_pred_binary)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_test_pred_binary)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(y_test, y_test_pred_binary)\n",
        "recall = recall_score(y_test, y_test_pred_binary)\n",
        "f1 = f1_score(y_test, y_test_pred_binary)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKnpA0cpF9CB",
        "outputId": "462ac7d3-055f-49c4-ed57-5a8142de582a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2016/2016 [==============================] - 294s 144ms/step - loss: 76101.5312 - accuracy: 0.6258 - val_loss: 0.5745 - val_accuracy: 0.8033\n",
            "Epoch 2/5\n",
            "2016/2016 [==============================] - 281s 140ms/step - loss: 5.5005 - accuracy: 0.7156 - val_loss: 0.5740 - val_accuracy: 0.8031\n",
            "Epoch 3/5\n",
            "2016/2016 [==============================] - 297s 148ms/step - loss: 0.5923 - accuracy: 0.7398 - val_loss: 0.5496 - val_accuracy: 0.7962\n",
            "Epoch 4/5\n",
            "2016/2016 [==============================] - 284s 141ms/step - loss: 0.5674 - accuracy: 0.7479 - val_loss: 0.5300 - val_accuracy: 0.7924\n",
            "Epoch 5/5\n",
            "2016/2016 [==============================] - 284s 141ms/step - loss: 346195.7500 - accuracy: 0.7309 - val_loss: 0.5100 - val_accuracy: 0.8074\n",
            "359/359 [==============================] - 14s 37ms/step\n",
            "Test Accuracy: 0.700985952360178\n",
            "Confusion Matrix:\n",
            "[[5094  513]\n",
            " [2914 2940]]\n",
            "Precision: 0.8514335360556038\n",
            "Recall: 0.5022207037922788\n",
            "F1 Score: 0.6317825292790373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings_list = [\"patlıcanları ince doğraki güzel pişsin\", \"kiminki kazanacak göreceğiz\", \"teyzeminki daha yeni duruyor\",\n",
        "                \"şöyle gelki yüzünü göreyim\", \"dağın yamaçlarındaki karlar eridi\"\n",
        "                ,\"ara sıra öyküler yazki yazın güçlensin\",\"ilaçlarını içki çabuk iyileş\",\"biliyorsunki bu film ödül aldı\",\"Ahemti tembihleki ağızından birşey kaçırmasın\",\n",
        "                \"Birşey biliyorki sesini çıkarmıyor\",\"Bu yarışmayı kazanabilecek misinki\",\n",
        "                \"eve döndüm baktımki gitmiş hemen telefona sarıldım\",\"Senki benim en iyi arkadaşımdın\",\n",
        "                \"gel görki ne bağ kırılmış ne bahçe\",\"bir şey biliyorki konuşuyor.\",\n",
        "                \"sen de onunla gitki, ona yardımcı olur\",\"Kırıkları toplaki, elimize batmasın\"\n",
        "                ,\"bizimle gelki biraz hava al\",\"yağmur yağmadıki mantarlar ortaya çıksın.\",\"tam evden çıkıyorduki telefon çaldı.\"]\n",
        "\n",
        "# Create a NumPy array from the list of strings\n",
        "test_labels=[0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "test_labels=np.array(test_labels)\n",
        "my_sentences = np.array(strings_list)"
      ],
      "metadata": {
        "id": "oiuJMJKQGBE5"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_my_sentences= np.array([vectorize(sentence) for sentence in my_sentences])\n",
        "#test_my_sentences = test_my_sentences.reshape((test_my_sentences.shape[0], 1, test_my_sentences.shape[1]))\n",
        "y_test_pred = model.predict(test_my_sentences)\n",
        "y_test_pred_binary = (y_test_pred > 0.5).astype(int)\n",
        "accuracy=accuracy_score(test_labels,y_test_pred_binary)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "# Calculate accuracy\n",
        "for i in range(len(my_sentences)):\n",
        "  print(f\"Test:{my_sentences[i]} Prediction:{y_test_pred_binary[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7kwtu9_GEST",
        "outputId": "5de5fb1c-85d4-4055-a6ec-5580f7303334"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 71ms/step\n",
            "Test Accuracy: 0.7\n",
            "Test:patlıcanları ince doğraki güzel pişsin Prediction:[1]\n",
            "Test:kiminki kazanacak göreceğiz Prediction:[1]\n",
            "Test:teyzeminki daha yeni duruyor Prediction:[1]\n",
            "Test:şöyle gelki yüzünü göreyim Prediction:[0]\n",
            "Test:dağın yamaçlarındaki karlar eridi Prediction:[0]\n",
            "Test:ara sıra öyküler yazki yazın güçlensin Prediction:[0]\n",
            "Test:ilaçlarını içki çabuk iyileş Prediction:[0]\n",
            "Test:biliyorsunki bu film ödül aldı Prediction:[0]\n",
            "Test:Ahemti tembihleki ağızından birşey kaçırmasın Prediction:[0]\n",
            "Test:Birşey biliyorki sesini çıkarmıyor Prediction:[0]\n",
            "Test:Bu yarışmayı kazanabilecek misinki Prediction:[0]\n",
            "Test:eve döndüm baktımki gitmiş hemen telefona sarıldım Prediction:[0]\n",
            "Test:Senki benim en iyi arkadaşımdın Prediction:[0]\n",
            "Test:gel görki ne bağ kırılmış ne bahçe Prediction:[1]\n",
            "Test:bir şey biliyorki konuşuyor. Prediction:[1]\n",
            "Test:sen de onunla gitki, ona yardımcı olur Prediction:[1]\n",
            "Test:Kırıkları toplaki, elimize batmasın Prediction:[1]\n",
            "Test:bizimle gelki biraz hava al Prediction:[0]\n",
            "Test:yağmur yağmadıki mantarlar ortaya çıksın. Prediction:[0]\n",
            "Test:tam evden çıkıyorduki telefon çaldı. Prediction:[0]\n"
          ]
        }
      ]
    }
  ]
}